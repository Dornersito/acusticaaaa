{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Presentación de proyecto</h1>\n",
    "<h2 style=\"text-align:center\"> ACUS220: Acústica Computacional con Python </h2>\n",
    "<h3 style=\"text-align:center\"> Segundo semestre 2024 </h3>\n",
    "\n",
    "<p style=\"text-align:center\"> <b>Profesor </b></p>\n",
    "<p style=\"text-align:center\">Dr. Víctor Poblete R. <br>\n",
    "<a href=\"mailto:vpoblete@uach.cl\">vpoblete@uach.cl</a><br>\n",
    "\n",
    "<p style=\"text-align:center\"> <b>Ayudantes </b></p>\n",
    "\n",
    "<p style=\"text-align:center\">Rafael Hayde P.<br>\n",
    "<a href=\"mailto:vpoblete@uach.cl\">rafael.penailillo@alumnos.uach.cl</a><br>\n",
    "\n",
    "<p style=\"text-align:center\">Esteban Vargas C.<br>\n",
    "<a href=\"mailto:vpoblete@uach.cl\">esteban.vargas01@alumnos.uach.cl</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Análisis de Sentimientos en Canciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "El presente proyecto tiene como objetivo desarrollar un sistema de clasificación de emociones en canciones utilizando aprendizaje automático. Mediante el análisis de características acústicas y espectrales, así como el entrenamiento de una red neuronal que aprende a asociar estas características con etiquetas de emoción, se busca explorar la relación entre las propiedades del audio y las emociones percibidas en la música.  \n",
    "\n",
    "Nuestro proyecto integra la API de Spotify con una red neuronal para predecir la emoción que una canción genera en el usuario. Esto nos permite aplicaciones como:\n",
    "\n",
    "- Recomendaciones Personalizadas: El modelo podría sugerir música que coincida con el estado emocional actual del usuario, adaptando las recomendaciones para mejorar su experiencia musical.\n",
    "\n",
    "- Playlists Emocionales: Se podrían generar playlists automáticas según las emociones deseadas, como relajación o motivación, ofreciendo listas dinámicas en plataformas como Spotify.\n",
    "\n",
    "- Terapia y Bienestar: En salud mental y musicoterapia, el modelo podría seleccionar canciones que ayudan a reducir estrés y ansiedad, promoviendo el bienestar emocional del usuario.\n",
    "\n",
    "Con este proyecto, buscamos que la música sea una experiencia aún más personalizada y conectada con las emociones.\n",
    "\n",
    "### Importancia del proyecto:\n",
    "La capacidad de clasificar las emociones de la música tiene múltiples aplicaciones en la industria del entretenimiento y la psicología. En plataformas de streaming, los sistemas de clasificación de emociones pueden mejorar la personalización de listas de reproducción según el estado de ánimo del usuario, y en contextos clínicos, puede ayudar a seleccionar música terapéutica adecuada para diferentes estados emocionales.\n",
    "\n",
    "\n",
    "### Descripción del dataset\n",
    "Para el desarrollo del modelo, se seleccionó un subset del dataset de Kaggle “278k Emotion Labeled Spotify Songs”, el cual proporciona información de aproximadamente 278.000 canciones, divididas en cuatro emociones principales: ‘sad’ (triste), ‘happy’ (feliz), ‘energetic’ (energético) y ‘calm’ (calma). Las características incluidas en el dataset son:\n",
    "\n",
    "- **Acousticness:** Mide el nivel de \"acústica\" de una canción, es decir, la probabilidad de que una pista sea acústica. Una canción con valores altos en esta métrica tiende a ser más instrumental y a utilizar menos efectos o instrumentos electrónicos. Valores cercanos a 1.0 indican alta probabilidad de ser acústica.   \n",
    "- **Danceability:** Evalúa qué tan adecuada es una canción para bailar, tomando en cuenta elementos como el tempo, la estabilidad del ritmo y la regularidad del beat. Las canciones con valores altos en danceability suelen tener un ritmo marcado, constante y fácil de seguir.  \n",
    "- **Energy:** Representa la intensidad y actividad de la canción, donde valores más altos indican un sonido más energético, rápido o fuerte. Es común encontrar valores elevados de energía en canciones de géneros como el rock o la música electrónica, en tanto que géneros como el jazz suelen tener valores más bajos.\n",
    "- **Instrumentalness:** Indica si la canción es instrumental o contiene pocos elementos vocales. Valores altos indican que la pista es predominantemente instrumental, mientras que valores cercanos a cero sugieren que contiene voces o letras claras.\n",
    "- **Liveness:** Mide la presencia de un público o la sensación de \"en vivo\" en la grabación. Valores altos en liveness indican una mayor probabilidad de que la pista haya sido grabada en directo frente a una audiencia, lo cual puede generar una atmósfera de concierto en la canción.\n",
    "- **Loudness:** Describe el volumen promedio de la canción en decibelios (dB). Es una característica que refleja el volumen general de la pista y, aunque puede estar relacionada con la energía, no es directamente proporcional. La loudness no afecta la emoción de manera directa pero puede influir en la percepción de intensidad.\n",
    "- **Speechiness:** Refleja la cantidad de palabras habladas en la canción. Canciones con altos valores de speechiness tienden a ser más habladas, como los audiolibros o discursos, mientras que las canciones con valores bajos suelen ser más melódicas.\n",
    "- **Valence:** Mide la positividad de una canción en términos emocionales. Canciones con valores altos en valence suelen evocar emociones positivas como alegría o euforia, mientras que valores bajos se asocian con tristeza, melancolía o tensión emocional.\n",
    "- **Tempo:** Refleja la velocidad o el ritmo de la canción, medido en beats por minuto (BPM). El tempo afecta la percepción de la canción, y los valores altos pueden estar relacionados con emociones energéticas o excitantes, mientras que tempos más bajos suelen asociarse a emociones calmadas o relajadas.\n",
    "\n",
    "Adicionalmente, se calcula el espectrograma Mel a partir de los primeros 5 segundos de cada canción debido a la intensiva demanda computacional de este cálculo, que retrasa el procesamiento si se aplicara a canciones completas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "Este proyecto se enfoca en desarrollar un modelo de aprendizaje profundo para predecir la emoción de una canción a partir de sus características. Los objetivos incluyen:\n",
    "\n",
    "- Crear un pipeline para el procesamiento de datos y la extracción de características acústicas.\n",
    "- Desarrollar una red neuronal profunda capaz de capturar las complejidades de las emociones en la música a partir de las características de entrada.\n",
    "- Evaluar el rendimiento del modelo usando métricas estándar y aplicar técnicas de mejora para optimizar el rendimiento.\n",
    "- Desarrollar una interfaz de conexion web con la API de Spotify\n",
    "- Integrar el modelo a la interfaz web, para seleccionar cualquier cancion  de Spotify y predecir la emocion que representa segun el modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodologia \n",
    "\n",
    "1. **Recopilación y Preprocesamiento de Datos**  \n",
    "  \n",
    "    **Recopilación del Dataset:** El dataset de Kaggle fue reducido a 400 canciones (100 por cada emoción) para optimizar el procesamiento. Cada canción está etiquetada en una de las siguientes categorías de emociones: ‘sad’ (0), ‘happy’ (1), ‘energetic’ (2) y ‘calm’ (3).\n",
    "\n",
    "    **Preprocesamiento de Audio y Extracción de Características:**\n",
    "\n",
    "    - Audio: Se extraen las características de la canción a partir del dataset. Además, las muestras de audio se extraen a partir de la API de `Spotify` para así calcular y generar el espectrograma Mel, que se obtiene solo de los primeros 5 segundos de cada canción.\n",
    "\n",
    "    - Etiquetas de Emoción: Las etiquetas se organizan en un vector de salida, siguiendo un esquema de codificación de clase única para cada emoción.\n",
    "\n",
    "\n",
    "2. **Construcción del Modelo** \n",
    "   \n",
    "    La arquitectura del modelo se basa en una red neuronal profunda compuesta por dos subredes principales:\n",
    "\n",
    "    - **Subred de Características Generales:** Esta subred procesa las características numéricas directas de la canción, como danceability, energy, valence, entre otras. Incluye una secuencia de capas densas que aprenden las correlaciones entre las características acústicas y las emociones.\n",
    "\n",
    "    - **Subred del Espectrograma Mel:** En esta subred, el espectrograma Mel es procesado a través de una serie de capas convolucionales (CNN) que extraen patrones temporales y de frecuencia asociados con las emociones.\n",
    "\n",
    "        Estas dos subredes se combinan en una capa intermedia que concatena las salidas de ambas, permitiendo que el modelo considere tanto las características generales como los patrones espectrales en la clasificación final. Finalmente, una capa softmax clasifica la salida en una de las cuatro emociones.\n",
    "\n",
    "\n",
    "3. **Entrenamiento y Validación**  \n",
    "    - **División de Datos:** El dataset se dividió en conjuntos de entrenamiento, validación y prueba. Se emplearon técnicas de ajuste de hiperparámetros como optimización de la tasa de aprendizaje, número de capas y tamaño de los filtros en la red convolucional.\n",
    "\n",
    "    - **Ajuste de Hiperparámetros:** Se aplicó optimización de tasa de aprendizaje y ajuste de hiperparámetros básicos para mejorar el rendimiento de la red. Aunque la clasificación inicial del modelo no fue óptima, se exploran técnicas avanzadas descritas en el cuadernillo para superar este desafío.\n",
    "\n",
    "4. **Integracion de la API de Spotify**\n",
    "    - **Conexion y consultas:** Se creo una aplicacion en la web Spotify for Developers para obtener un token de acceso a la API, con la cual, mediante un script en Javascript, se pueden hacer consultas de canciones y extraer las caracteristicas que se precisan.\n",
    "    - **Integracion con el modelo:** Se crea una conexion entre la aplicacion web y el modelo de red neuronal, para que en el momento en el que se seleccione una cancion, se le envien a la red neuronal las caracteristicas necesarias para hacer la prediccion. \n",
    "\n",
    "5. **Interfaz grafica**\n",
    "    - **Frontend:** Se desarrollo una interfaz grafica en la cual un usuario puede buscar su cancion por nombre o artista, y  seleccionar una emocion que le haga sentir al usuario. En base a la cancion, se le enviaran las caracteristicas al modelo entrenado y este predecira la emocion que le hace sentir al usuario, luego desplegando la respuesta del modelo y la del usuario en pantalla.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo\n",
    "La selección del espectrograma Mel en dB se fundamentó en la experiencia previa de un integrante del grupo, quien trabajó en un proyecto donde clasificaba sonidos submarinos, específicamente ruidos provenientes del mar. En ese proyecto, el uso del espectrograma Mel resultó en resultados prometedores, sugiriendo su capacidad para distinguir características de audio complejas y variadas. Esta experiencia nos motivó a explorar su aplicabilidad en la clasificación de canciones, dado que esta representación capta información de frecuencia y temporalidad de una forma particularmente adecuada para el análisis de señales de audio.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"sounds.png\" alt=\"Espectrorgama\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "\n",
    "### ¿Qué es el Espectrograma Mel?\n",
    "El espectrograma Mel es una representación visual de una señal de audio en el dominio de la frecuencia, con la particularidad de que usa una escala mel, que se ajusta mejor a la percepción humana de los sonidos. En el espectrograma Mel, la intensidad del sonido en cada frecuencia se representa en decibelios (dB) a través de colores, siendo común ver las frecuencias bajas en la parte inferior de la imagen y las altas en la parte superior.\n",
    "\n",
    "Un espectrograma Mel permite ver cómo cambian las frecuencias a lo largo del tiempo, proporcionando una “huella dactilar” de la señal de audio que es extremadamente útil para capturar patrones específicos asociados a características emocionales o de timbre en canciones. Esta representación facilita a la red neuronal identificar diferencias en la textura o el ritmo de la música que podrían ser indicativos de emociones particulares.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"espectrograma.png\" alt=\"Espectrorgama\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "\n",
    "### Estructura de la Red Neuronal\n",
    "Para la creación de nuestro modelo de clasificación, decidimos integrar tanto las características generales de cada canción como su espectrograma Mel en dB. Esto nos llevó a diseñar una red neuronal compuesta por dos subredes:\n",
    "\n",
    "- Subred para características de audio: En esta primera subred, procesamos directamente las características generales de la canción, como acousticness, energy, valence, etc., extrayendo patrones a partir de estos atributos cuantitativos.\n",
    "\n",
    "- Subred para el espectrograma Mel: En la segunda subred, ingresamos el espectrograma Mel de cada canción, donde la red puede aprender a identificar patrones temporales y de frecuencia relacionados con las emociones.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"red.png\" alt=\"Espectrorgama\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Una vez que cada subred extrae sus respectivas características, ambas se combinan para realizar la predicción final de la emoción de la canción. Aunque no tenemos un diseño específico de red adecuado para este problema en la literatura existente, decidimos implementar esta estructura al considerar que permite un enfoque lógico y racional al combinar dos tipos de información valiosa para la clasificación.\n",
    "\n",
    "\n",
    "### Interfaz Grafica\n",
    "\n",
    "Como primera version de interfaz grafica se encuentra un panel en el cual se puede escribir el nombre de un artista o cancion y un boton con el cual se podran obtener las caracteristicas de la cancion. \n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"spotify 1.png\" alt=\"sporify1\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "Luego se despliegan 5 opciones de canciones, las cuales son las relacionadas al nombre del artista o cancion que se ingreso. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"spotify 2.png\" alt=\"spotify2\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "Al seleccionar una de las 5 opciones, se podra oprimir el boton de obtener caracteristicas.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"spotify 3.png\" alt=\"spotify3\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "Esto despliega en pantalla las caracteristicas de la cancion, las cuales son necesarias para que el modelo pueda funcionar.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"spotify 4.png\" alt=\"spotify4\" width=\"800\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados\n",
    "\n",
    "La matriz de confusión revela un patrón claro de errores en las predicciones, especialmente en las clases 0 y 1, donde hay una alta cantidad de falsos positivos y negativos. Esto sugiere que el modelo tiene dificultades para distinguir adecuadamente entre las emociones de estas clases. Además, el reporte de clasificación indica que mientras la clase 2 tiene un buen recall (94%) y f1-score (0.74), las clases 0 y 1 muestran un desempeño limitado, con f1-scores de 0.43 y 0.42, respectivamente. La precisión global es del 55%, lo que indica que el modelo no está generalizando bien en el conjunto de validación.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"matrix.png\" alt=\"con_matrix\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "\n",
    "El comportamiento de las curvas de pérdida y precisión observadas en las épocas de entrenamiento y validación sugiere que el modelo está sobreajustado (overfitting) a los datos de entrenamiento, logrando alta precisión en este conjunto pero fallando en predecir correctamente en el conjunto de prueba. Esto es una clara señal de que el modelo ha memorizado patrones específicos del conjunto de entrenamiento en lugar de aprender características generales que puedan aplicarse a nuevos datos.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"curvas.png\" alt=\"curvas\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Para mejorar el rendimiento del modelo, en futuras iteraciones se explorarán estrategias como:\n",
    "- Aumentar el conjunto de datos: Incluir una mayor variedad de canciones para ayudar al modelo a capturar una gama más amplia de características emocionales.\n",
    "- Agregar más características: Incorporar atributos adicionales que puedan aportar mayor información al modelo y mejorar su capacidad de generalización.\n",
    "- Ajustar la arquitectura de la red: Implementar técnicas de regularización (como dropout o L2 regularization) y modificar la estructura de la red para reducir el sobreajuste.  \n",
    "\n",
    "Con estas mejoras, se espera construir un modelo más robusto y preciso en futuras entregas, capaz de clasificar emociones en canciones de manera confiable y con mayor precisión\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados Esperados\n",
    "\n",
    "- **Modelo de Clasificación de Emociones:** Se espera obtener una precisión del 70-80% en la clasificación de las canciones en sus categorías emocionales. Esto incluye tanto la evaluación en el dataset de prueba como en nuevas canciones que no formaron parte del entrenamiento.\n",
    "\n",
    "- **Interpretación y Visualización de Resultados:** Se espera poder visualizar los resultados del modelo mediante gráficos y diagramas que permitan entender la clasificación en función de las características del audio. Esto podría incluir una interfaz interactiva que muestre el cambio en la emoción percibida a lo largo de una canción.\n",
    "\n",
    "- **Análisis de Desempeño:** Se creará un informe que resuma las métricas de rendimiento, los principales retos encontrados durante el desarrollo y las oportunidades de mejora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discusión\n",
    "\n",
    "La construcción de este modelo de clasificación emocional en canciones ha puesto de manifiesto varias limitaciones y oportunidades para futuros entregables. La restricción de tiempo en el cálculo de espectrogramas y la dificultad en la clasificación de ciertas emociones indican que agregar análisis adicionales, como el procesamiento de otras características, podría mejorar los resultados.\n",
    "\n",
    "Además, el dataset reducido a solo 400 canciones limita la capacidad de generalización del modelo. Incluir un mayor número de muestras y explorar técnicas de aprendizaje de transferencias serían enfoques potenciales para futuros trabajos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Este proyecto representa un primer paso hacia la construcción de modelos de clasificación emocional en música, aprovechando las características acústicas y el análisis espectral. A pesar de los desafíos encontrados, los resultados esperados reflejan un avance en la comprensión de cómo diferentes patrones en el audio pueden estar relacionados con emociones percibidas, y establecen las bases para aplicaciones futuras en sistemas de recomendación y análisis emocional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "- Kaggle Dataset: 278k Emotion Labeled Spotify Songs – https://www.kaggle.com/datasets/abdullahorzan/moodify-dataset/data. Un dataset de canciones etiquetadas con emociones que contiene una gran cantidad de caracteristicas extraidas de spotify, utilizado para entrenar modelos de machine learning en el reconocimiento de sentimientos en música.\n",
    "\n",
    "- Librosa: Python package para análisis de audio y extracción de características. https://librosa.org/.\n",
    "\n",
    "- TensorFlow/Keras: Frameworks populares para el desarrollo de modelos de deep learning que podrían ser útiles para la construcción del modelo de clasificación de emociones. https://www.tensorflow.org/.\n",
    "\n",
    "- PyTorch: Otro framework de deep learning ampliamente utilizado. https://pytorch.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analisis-voz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
